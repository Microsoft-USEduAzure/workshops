{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Azure Machine Learning Workshop\n",
    "## AutoML Classification\n",
    "\n",
    "In this workshop we will use the [Azure Machine Learning SDK for Python](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py) as well as Automated Machine Learning services from Azure to predict and flag students who are at risk of dropping a course. \n",
    "\n",
    "### Prerequisites\n",
    " - Access to an Azure Subscription\n",
    " - Completion of the [Azure Fundamentals Lab](https://aka.ms/edu/Azure101)\n",
    " - [Create an Azure Machine Learning Services Workspace](https://github.com/Microsoft-USEduAzure/workshops/blob/master/intro-to-automl/create-an-azure-machine-learning-workspace.md)\n",
    " - [Create a Notebook VM](https://github.com/Microsoft-USEduAzure/workshops/blob/master/intro-to-automl/create-a-notebook-vm.md)\n",
    " \n",
    " \n",
    "### Task List\n",
    "- [Connect to the Azure Machine Learning Services Workspace](#Connect-to-the-Azure-Machine-Learning-Services-Workspace)\n",
    "- [Load data from Azure Blob Storage](#Load-data-from-Azure-Blob-Storage)\n",
    "- [Transform and prepare data](#Transform-and-prepare-data)\n",
    "- [Configure AutoML Experiment](#Configure-AutoML-Experiment)\n",
    "- [Run AutoML Experiment & Explore Results](#Run-AutoML-Experiment-&-Explore-Results)\n",
    "- [Retreive and Test AutoML model](#Retreive-and-Test-AutoML-model)\n",
    "- [Apply AutoML model to whole dataset](#Apply-AutoML-model-to-whole-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the Azure Machine Learning Services Workspace\n",
    "[Back to Top](#Task-List)  \n",
    "\n",
    "For Azure AutoML, you will need to create an **Experiment**, which is a named object in your Machine Learning Workspace used to run experiments.\n",
    "\n",
    "1. First, we will import the Azure Machine Learning Service and Logging libraries and validate SDK version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next, we define variables with the values we retrived for our **Subscription ID**, **Resource Group Name**, and **Machine Learning Workspace Name.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = '<Your Subscription ID>'\n",
    "resource_group  = '<Your Resource Group Name>'\n",
    "workspace_name  = '<Your Workspace Name>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We will now use those variables to ensure that the Machine Learning Workspace exists, and we are able to connect to it.\n",
    "\n",
    "    If the result of the following cell is 'Workspace not found,' review the variables we defined in the previous step to ensure they match the values from [Create an Azure Machine Learning Services Workspace](#Create-an-Azure-Machine-Learning-Services-Workspace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "try:\n",
    "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "    ws.write_config()\n",
    "    print('Library configuration succeeded')\n",
    "except:\n",
    "    print('Workspace not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from Azure Blob Storage\n",
    "[Back to Top](#Task-List)  \n",
    "\n",
    "1. Now we will connect to Azure Blob Storage. To do so, we will first import the Azure Storage library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlockBlobService\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next, we will define variables that correspond to our storage account, and the location of the data we'll be creating the model from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageaccount_name = '<Your Storage Account Name>'\n",
    "storageaccount_key = '<Your Storage Account Key>'\n",
    "\n",
    "local_filename = './MLInput.tsv'\n",
    "container_name = 'sources'\n",
    "blob_name = 'MLInput.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We'll use the variables from step 2 to create a connection to Blob storage and retrive the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download from blob\n",
    "\n",
    "blob_service=BlockBlobService(account_name=storageaccount_name,account_key=storageaccount_key)\n",
    "blob_service.get_blob_to_path(container_name,blob_name,local_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Now we'll load the blob data into a [Pandas dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html). A Data frame is a two-dimensional data structure, i.e., data is aligned in a tabular fashion in rows and columns. Pandas DataFrame consists of three principal components, the data, rows, and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "studentDropDF = pd.read_csv(local_filename, sep='\\t', index_col=False)\n",
    "studentDropDF.index.name = 'Index'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Now that the data is stored in a Data Frame, we can display it to explore the data and gain context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(studentDropDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform and prepare data\n",
    "[Back to Top](#Task-List)  \n",
    "\n",
    "1. First, we will split the data into two data frames. One for indepedent variables (studentDropDF_X) and one for the depedent variable (studentDropDF_y.)\n",
    "\n",
    "    For our analysis the depdent variable will be **Dropped**, where 1 indicates a course was dropped and 0 indicates the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studentDropDF_x = studentDropDF.drop(\"Dropped\", axis=1)\n",
    "studentDropDF_y = studentDropDF.filter([\"Dropped\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now we will import train_test_split from the sklearn library so that we may segment the dataframes further into two sets of data:\n",
    "\n",
    "    - **x_train** and **y_train**: Will have 70% of all the data and be used to train our model.\n",
    "    - **x_test** and **y_test**: Will have 30% of all the data and be used to validate the result of our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(studentDropDF_x,studentDropDF_y, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove comment to view dataframes\n",
    "#display(x_train)\n",
    "#display(x_test)\n",
    "#display(y_train)\n",
    "#display(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure AutoML Experiment\n",
    "[Back to Top](#Task-List)  \n",
    "\n",
    "\n",
    "Begin to configure the AutoML by instantiating an AutoMLConfig object to define the settings and data used to run the expirement ([documentation link](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train#configure-your-experiment-settings)). \n",
    "\n",
    "1. We will first define the Experiment Name, which will correspond to generated expirement in your Machine Learning Workspace. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'automated-ml-classification'\n",
    "project_folder = './automated-ml-classification'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next, import AutoMLConfig from the AzureML Library, define our automl_settings, and instantiate the AutoMLConfig object.\n",
    "\n",
    "\n",
    "\n",
    "| Setting | Description   |\n",
    "|------|------|\n",
    "|**task**  | classification or regression|\n",
    "|**primary_metric**  | Determines the metric to be used during model training for optimization.|\n",
    "|**iterations**  | Number of iterations. In each iteration AutoML trains a specific pipeline with the data.|\n",
    "|**iteration_timeout_minutes**  | Define a time limit in minutes per each iteration.|\n",
    "|[**preprocess**](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-automated-ml#preprocess) | Advanced preprocessing and featurization (e.g. missing value imputation, categorization)|\n",
    "|**X** | Indepdent variable array |\n",
    "|**Y** | Depdent variable array|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "\n",
    "automl_settings = {\n",
    "  \"iteration_timeout_minutes\" : 5,\n",
    "  \"iterations\" : 5,\n",
    "  \"primary_metric\" : 'accuracy',\n",
    "  \"preprocess\" : True,\n",
    "  \"verbosity\" : logging.INFO,\n",
    "  \"n_cross_validations\": 5\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(\n",
    "                             task = 'classification',\n",
    "                             debug_log = 'automated_ml_errors.log',\n",
    "                             path = project_folder,\n",
    "                             X = x_train,\n",
    "                             y = y_train,\n",
    "                             **automl_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run AutoML Experiment & Explore Results\n",
    "[Back to Top](#Task-List)  \n",
    "\n",
    "1. Now we will instantiate an Expirement object using the AutoMLConfig object we created, and submit the expirement to **run** in Azure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Depending on compute, complexity, and the number of iterations, the expirement may take some time to run; however, we will be able to monitor the progress of the run by turning **show_output** to equal **True**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Explore the result of the expirement by going to the [Azure portal](https://portal.azure.com), or by levarging the **RunDetails** widget.\n",
    "\n",
    "    After completing the first iteration, an auto-updating graph and table will be shown. The widget will refresh once per minute, so you should see the graph update as child runs complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. You can also use SDK methods to fetch all the child runs and see individual metrics that we log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children = list(run.get_children())\n",
    "metricslist = {}\n",
    "for r in children:\n",
    "    properties = r.get_properties()\n",
    "    metrics = {k: v for k, v in r.get_metrics().items() if isinstance(v, float)}\n",
    "    metricslist[int(properties['iteration'])] = metrics\n",
    "\n",
    "rundata = pd.DataFrame(metricslist).sort_index(1)\n",
    "rundata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreive and Test AutoML model\n",
    "[Back to Top](#Task-List)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving models\n",
    "\n",
    "We can now use the **get_output** method to return the best run and the fitted model. The Model includes the pipeline and any pre-processing. Overloads on **get_output** allow you to retrieve the best run and fitted model for **any** logged metric or for a particular iteration.\n",
    "\n",
    "1. We can now retrieve models based on any metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_metric = \"log_loss\"\n",
    "best_run, fitted_model = run.get_output(metric = lookup_metric)\n",
    "print(best_run)\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Retrieve a specific iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 3\n",
    "third_run, third_model = run.get_output(iteration = iteration)\n",
    "print(third_run)\n",
    "print(third_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Or we select the best pipeline from our iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = run.get_output()\n",
    "print(best_run)\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We can also explore the best run further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing models\n",
    "\n",
    "1. We'll test our model by using the **x_test** dataframe and receive our model's prediction and the probability of the prediction. \n",
    "\n",
    "    - Prediction will be a one dimensional array of either 1 (predicted to drop) or 0 (predicted to not drop)\n",
    "    - Since our target is (0,1), probabliy returns a two dimensional array. The first index refers to the probability that the data belong to class 0, and the second refers to the probability that the data belong to class 1. In this scenario, we are only concerned about the second index since it correlates to the risk of dropping a course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = fitted_model.predict(x_test)\n",
    "y_risk = fitted_model.predict_proba(x_test)\n",
    "\n",
    "## Set y_risk probability of dropping the course\n",
    "y_risk = y_risk[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We can now compare against random class drops from the **y_test** array to the predicted class drops from the **y_predict** and **y_risk** arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = y_test.values.flatten().tolist()\n",
    "\n",
    "import numpy as np\n",
    "for i in np.random.choice(len(y_actual), 10, replace = False):\n",
    "\n",
    "    #print(i)\n",
    "    predicted = y_predict[i]\n",
    "    probability = y_risk[i]\n",
    "    actual = y_actual[i]\n",
    "    \n",
    "    output = \"Actual value = {}  Predicted value = {} (Risk: {:.3f}) \".format(actual, predicted, probability)\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We can also use a confusion matrix to compare true values versus predicted values. The number of correct ('True Positives' or 'True Negatives') and incorrect predictions ('False Positives' or 'False Negatives') are summarized with count values and broken down by each class. This is the key to the confusion matrix.\n",
    "\n",
    "![Confusion Matrix](media/5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a confusion matrix to see how many samples from the test set are classified correctly. \n",
    "#Notice the misclassified value for the incorrect predictions:\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mx = confusion_matrix(y_actual, y_predict)\n",
    "print(conf_mx)\n",
    "print('Overall accuracy:', np.average(y_predict == y_actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply AutoML model to whole dataset\n",
    "[Back to Top](#Task-List)  \n",
    "\n",
    "Now that we've verified the model's accuracy, we can begin to apply it.  \n",
    "\n",
    "For this scenario, we'll simply reset the y_predict and y_risk to the whole **studentDropDF_x** dataframe. However, you can retrive the model as a pickle (.pkl) file and deploy the model to a power cluster, containers, or other compute resource. Follow [this documentation link](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First, we compute the prediction and probability of dropped a course against the **studentDropDF_x** dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Entire Dataset\n",
    "\n",
    "y_predict = fitted_model.predict(studentDropDF_x)\n",
    "y_risk = fitted_model.predict_proba(studentDropDF_x)\n",
    "y_risk = y_risk[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We can also use the probability of dropping a course to set a flag of Low, Medium, or High correlated to the probability's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "risk_list = []\n",
    "\n",
    "for i in y_risk:\n",
    "    risk = \"Medium\"\n",
    "    if i < .25:\n",
    "        risk = \"Low\"\n",
    "    elif i >= .5:\n",
    "        risk = \"High\"\n",
    "    risk_list.append(risk)\n",
    "\n",
    "risk_array = np.array(risk_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We'll now define a new dataframe to report off of, with de-identified student information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReportingDF = studentDropDF_x.filter([\"StudentID\",\"TermID\",\"SubjectID\",\"CatalogID\",\"ClassID\",\"SectionID\",\"EnrollDate\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Next, append y_predict as DropoutFlag (0 or 1,) y_risk as DropoutProbability (0.0 - 1.0,) and risk_array as RiskCategory (Low, Medium, High) to the **ReportingDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReportingDF['DropoutFlag'] = y_predict\n",
    "ReportingDF['DropoutProbability'] = y_risk\n",
    "ReportingDF['RiskCategory'] = risk_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We can now display this dataframe to get a sense of the data at a glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ReportingDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. This dataframe can then be directly insterted into a database for reporting and analytics, or we can do further analysis with visualizations by using the matplotlib and seaborn libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "sns.distplot(y_risk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(ReportingDF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
